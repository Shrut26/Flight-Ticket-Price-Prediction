# -*- coding: utf-8 -*-
"""Flight Ticket Price Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Rd9-rKpfqajdJLjTm5Jlv7F9moJJ_uyp

<div class = "container" align = center>
<big><h1 align=center> Flight Ticket Price Prediction </h1></big>

</div>

#**Useful Library Import**
"""

import pandas as pd 
import numpy as np 
import seaborn as sns 
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score,confusion_matrix, mean_squared_error, mean_squared_log_error
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split

"""#**Import Useful Models**"""

from xgboost import XGBRegressor
import lightgbm as lgbm
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_log_error , mean_squared_error , mean_absolute_error , r2_score
from sklearn.model_selection import cross_val_score, RandomizedSearchCV
from sklearn.decomposition import PCA

"""#**Reading Dataset**"""

data = pd.read_csv('https://docs.google.com/spreadsheets/d/e/2PACX-1vTVF_Hy83R3lxjXAOfhvAvVMf12s7SYbNIoUAmqcclSpolebRWUkjFONleBN59m8fkpLVhTZbmPkVUT/pub?output=csv')
data

"""##Dataset Describition

The given dataset has 11 columns which consists of :
1. Airline -> name of airline
2. Date of Journey -> starting date of journey
3. Source -> starting station of journey
4. Destination -> End station of journey
5. Route -> Stations which will occur between Source and destination
6. Dep_Time -> starting time of journey
7. Arrival_time -> end time of journey
8. Duration -> Time duration of the journey
9. Total_stops -> stops taken by the flight during the journey
10. Additional_info -> additional information
11. Price -> Ticket Price

#**Preprocessing of Dataset and constructing proper dataset**

Exploratory Data Analysis

checking dataset describe
"""

data.describe()

"""Checking for any  null values"""

data.isnull().sum()

"""Dropped null rows"""

data = data.dropna()

"""Checking data types of columns and looking for their correctness"""

data.dtypes

"""[ ' Date of Journey' , ' Dep Time ' , ' Arrival Time ' ] are objects, so these columns needs to be converted to date and time."""

for col in ['Date_of_Journey','Dep_Time', 'Arrival_Time']:
  data[col] = pd.to_datetime(data[col])

data.dtypes

"""Extracting date and month from Date_of_journey, as a result there would be no use of 'Date_of_journey' column so it will be dropped."""

data['journey_day'] = data['Date_of_Journey'].dt.day
data['journey_month'] = data['Date_of_Journey'].dt.month

data = data.drop(columns = ['Date_of_Journey'], axis = 1)

"""Similarly Extracting Hours and minutes from Departure Time and Arrival Time"""

data['Dep_Time_hour'] = data['Dep_Time'].dt.hour
data['Dep_Time_min'] = data['Dep_Time'].dt.minute

data['Arrival_Time_hour'] = data['Arrival_Time'].dt.hour
data['Arrival_Time_min'] = data['Arrival_Time'].dt.minute

data

data = data.drop(columns = ['Dep_Time','Arrival_Time'], axis = 1)

"""Separate hours and minutes from duration"""

def duration_convertion(x):
    if len(x.split()) == 2:
        one = int(x.split()[0][:-1])
        two = int(x.split()[1][:-1])
        return one * 60 + two
    else:
        return int(x[:-1]) * 60

data["Duration"] = data["Duration"].apply(duration_convertion)

data

data.reset_index(inplace = True)
data.drop("index",inplace = True, axis = 1)

"""Splitting the via column and removing all '->' from it and making each inserting each station in a separate column."""

via = []
for i in range(len(data)):
    via.append(data["Route"][i].split("â†’"))   # making a list of list of routes station so that each station in route can be inserted into a different column
# via

lens = []
for i in via:
    lens.append((len(i) , i))  
# max(lens)[0]

for i in via:
    while len(i) <= 5:
        i.append("None")

"""Inserting different station in separate columns"""

data['Route1'] = [row[0] for row in via]
data['Route2'] = [row[1] for row in via]
data['Route3'] = [row[2] for row in via]
data['Route4'] = [row[3] for row in via]
data['Route5'] = [row[4] for row in via]
data['Route6'] = [row[5] for row in via]

"""Now we don't neeed Route column"""

data = data.drop(columns = ['Route'], axis = 1)
data

data['Additional_Info'].value_counts()

route_columns = ['Route1','Route2','Route3','Route4','Route5','Route6']
for route in route_columns:
  data[route] = LabelEncoder().fit_transform(data[route])
data

data['Total_Stops'].value_counts()

"""Encoding Total_stops column"""

for stop in range(len(data)):
  if(data['Total_Stops'][stop] == 'non-stop'):
    data['Total_Stops'][stop] = 0
  elif(data['Total_Stops'][stop] == '1 stop'):
    data['Total_Stops'][stop] = 1
  elif(data['Total_Stops'][stop] == '2 stops'):
    data['Total_Stops'][stop] = 2
  elif(data['Total_Stops'][stop] == '3 stops'):
    data['Total_Stops'][stop] = 3
  elif(data['Total_Stops'][stop] == '4 stops'):
    data['Total_Stops'][stop] = 4
data

"""Since it is obvious that Source and the Route1 are to be same, so we Route1 column is dropped"""

data = data.drop(columns = ['Route1'], axis = 1)

"""Getting dummies for the categorical columns"""

data = pd.get_dummies(data , columns = ["Airline" , "Source" , "Destination" , "Additional_Info"] , 
                      prefix = ["Airline" , "Source" , "Destination" , "Additional_Info"] , drop_first = True)

data.columns

data['Total_Stops'] = data['Total_Stops'].astype(int)

data.dtypes

continuous_columns = ['Duration','Total_Stops','journey_day','journey_month','Dep_Time_hour','Dep_Time_min','Arrival_Time_hour','Arrival_Time_min']
data[continuous_columns] = StandardScaler().fit_transform(data[continuous_columns])

data

"""##Checking for any outliers in the dataset"""

def plot(data,col):
    fig,(ax1,ax2)=plt.subplots(2,1)
    sns.distplot(data[col],ax=ax1)
    sns.boxplot(data[col],ax=ax2)
plot(data,'Price')

"""As from the above dataset we can clearly see the outliers , so I will replace all the prices > 40000 with the median value of price."""

data['Price']=np.where(data['Price']>=40000,data['Price'].median(),data['Price'])

plot(data,'Price')

"""##Train test Split"""

X = data.drop(columns = ['Price'], axis = 1)
Y = data[['Price']]

x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size = 0.3, random_state = 42)
x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size = 2/3, random_state = 42)

x_train.reset_index(inplace = True)
x_train.drop("index",axis = 1, inplace = True)

y_train.reset_index(inplace = True)
y_train.drop("index",axis = 1, inplace = True)

x_val.reset_index(inplace = True)
x_val.drop("index",axis = 1, inplace = True)

y_val.reset_index(inplace = True)
y_val.drop("index",axis = 1, inplace = True)

x_test.reset_index(inplace = True)
x_test.drop("index",axis = 1, inplace = True)

y_test.reset_index(inplace = True)
y_test.drop("index",axis = 1, inplace = True)

"""#Applying Models

##Modelling
"""

Models = ['XGBRegressor','RandomForestRegressor','Linear Regressor','KNeighborsRegressor','AdaBoostRegressor','LGBMRegressor']
r2_scores = []
training_scores = []
testing_scores = []
cross_val_scores = []
mse_scores = []
log_scores = []

training_x = pd.concat([x_train,x_val], axis = 0)
training_y = pd.concat([y_train,y_val], axis = 0)

def modelling(model, model_name):
  print("--------------------------------------Applying " + model_name + " on the given dataset----------------------------------------")
  regressor = model
  regressor.fit(x_train, y_train)
  cross_validation_score = cross_val_score(estimator = regressor, X = training_x, y = training_y, cv = 5, scoring = "r2")
  preds = regressor.predict(x_test)
  score = max(0 , 100 * r2_score(y_test , preds))
  log_error = mean_squared_log_error(y_test, preds)
  print(f"Training score : {regressor.score(x_train , y_train):.4f}")
  print(f"Test Score : {regressor.score(x_test , y_test):.4f}")
  print(f"K-fold accuracy : {np.mean(cross_validation_score):.4f}")
  print(f"Standard Deviation of Accuracies in k-fold : {np.std(cross_validation_score):.4f}")
  print(f"R2 - Score : {score:.4f}")
  print(f"Mean square log error: {log_error:.4f}")
  training_scores.append(regressor.score(x_train , y_train))
  testing_scores.append(regressor.score(x_test , y_test))
  cross_val_scores.append(np.mean(cross_validation_score))
  r2_scores.append(score)
  log_scores.append(log_error)
  print()

modelling(XGBRegressor(random_state = 42 , objective ='reg:squarederror'), "XGBRegressor")
modelling(RandomForestRegressor(random_state = 42), "RandomForestRegressor")
modelling(LinearRegression(), "LinearRegressor")
modelling(KNeighborsRegressor(), "KNeightborsRegressor")
modelling(AdaBoostRegressor(random_state = 42), "AdaboostRegressor")
modelling(lgbm.LGBMRegressor(), "LGBMRegressor")

score = pd.DataFrame({'Model': Models, 'Training score': training_scores, 'Testing Score': testing_scores, 'R2 - Score' : r2_scores,'Cross_val_scores' : cross_val_scores,'Mean square log error': log_scores})
score.style.background_gradient(high=1,axis=0)

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "R2 - Score" , data = score)
plt.title("Model Comparision based on R2_Score");

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "Mean square log error" , data = score)
plt.title("Model Comparision based on Mean square log error");

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "Cross_val_scores" , data = score)
plt.title("Model Comparision based on Cross_val_scores");

"""We can clearly see that Random Forest Regressor is winning"""

random_search_testing_scores = []
msle_testing_scores = []

"""##Lets find the best parameter for RFC and tune it with hyperparameters using RandomSearch

"""

param_dist = {
'n_estimators': list(range(10, 300, 5)),
'min_samples_leaf': list(range(1, 50)),
'max_depth': list(range(2, 30)),
'max_features': ['auto', 'sqrt'],
'bootstrap': [True, False]
}

rfc = RandomForestRegressor()
random_search = RandomizedSearchCV(rfc, param_distributions = param_dist, n_iter = 10, scoring = 'r2', n_jobs = 1, cv = 5, verbose = 5)

random_search.fit(x_train , y_train)

random_search.best_estimator_

random_search.best_params_

"""Testing accuracy from RandomForestRegressor"""

preds = random_search.predict(x_test)
testing_score = max(0 , 100 * r2_score(y_test , preds))
msle = max(0,mean_squared_log_error(y_test,preds))
print(f"The R2-score by applying tuned RandomForestRegressor is : {testing_score:.4f}")
print(f"The MSLE score by applying tuned RandomForestRegressor is : {msle:.4f}")
random_search_testing_scores.append(testing_score)
msle_testing_scores.append(msle)

"""##Lets find the best parameter for LightGBM and tune it with hyperparameters using RandomSearch"""

params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ,9],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
}

lgb = lgbm.LGBMRegressor()
random_search = RandomizedSearchCV(lgb, param_distributions = params, n_iter = 8, scoring = 'r2', n_jobs = 1, cv = 5, verbose = 3)

random_search.fit(x_train , y_train)

random_search.best_estimator_

random_search.best_params_

"""Testing accuracy from LGBMRegressor"""

preds = random_search.predict(x_test)
testing_score = max(0 , 100 * r2_score(y_test , preds))
msle = max(0,mean_squared_log_error(y_test,preds))
print(f"The testing score by applying tuned LGBMRegressor is : {testing_score:.4f}")
print(f"The MSLE score by applying tuned RandomForestRegressor is : {msle:.4f}")
random_search_testing_scores.append(testing_score)
msle_testing_scores.append(msle)

"""##Lets find the best parameter for LightGBM and tune it with hyperparameters using RandomSearch"""

params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ,9],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
}

xgb = XGBRegressor()
random_search = RandomizedSearchCV(xgb, param_distributions = params, n_iter = 8, scoring = 'r2', n_jobs = 1, cv = 5, verbose = 3)

random_search.fit(x_train , y_train)

random_search.best_estimator_

random_search.best_params_

preds = random_search.predict(x_test)
testing_score = max(0 , 100 * r2_score(y_test , preds))
msle = max(0,mean_squared_log_error(y_test,preds))
print(f"The testing score by applying tuned XGBrRegressor is : {testing_score:.4f}")
print(f"The MSLE score by applying tuned RandomForestRegressor is : {msle:.4f}")
random_search_testing_scores.append(testing_score)
msle_testing_scores.append(msle)

"""##Comparing all three models"""

final_testing_score = pd.DataFrame({'Model': ['RandomForestRegressor','LightGBM','XGBRegressor'], 'R2-score': random_search_testing_scores, 'MSLE':msle_testing_scores})
final_testing_score.style.background_gradient(high=1,axis=0)

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "R2-score" , data = final_testing_score)
plt.title("Model Comparision based on Testing Score based on R2- score");

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "MSLE" , data = final_testing_score)
plt.title("Model Comparision based on MSLE");

"""#**Applying PCA on dataset**

##pca
"""

pca = PCA()
pca.fit(X)

pca_components = list(pca.explained_variance_ratio_)
total_components = list(range(0,X.shape[1]))
plt.bar(total_components, pca_components)
plt.xlabel("PCA components")
plt.ylabel("Amount of variance")
plt.title("PCA visualisation")

"""From the above graph it is confirmed that first two principle components contributes to the most of variance. Hence lets see the exact value of variance which we are able to capture by taking first two PCA's."""

normalised_variance_of_pca = []
normalised_variance_of_pca.append(pca_components[0])
for i in range(1,len(pca_components)):
  normalised_variance_of_pca.append(normalised_variance_of_pca[i-1] + pca_components[i])
plt.bar(total_components, normalised_variance_of_pca)
plt.xlabel("PCA components")
plt.ylabel("Total amount of variance by taking these much PCA'S")
plt.title("PCA visualisation")

normalised_variance_of_pca

"""So from above values if we take n_components = 7 the we are able to capture about 98 % of variance."""

values = pca.transform(X)
pca_data = pd.DataFrame(columns = ['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6','PCA7','PCA8','PCA9','PCA10'], data = values[:,:10])
pca_data['Price'] = Y
pca_data

X_pca = data.drop(columns = ['Price'], axis = 1)
Y_pca = data[['Price']]
x_train, x_test, y_train, y_test = train_test_split(X_pca,Y_pca,test_size = 0.3, random_state = 42)

"""Training the above 3 models and tuning them with hyperparameters"""

param_dist = {
'n_estimators': list(range(10, 300, 5)),
'min_samples_leaf': list(range(1, 50)),
'max_depth': list(range(2, 30)),
'max_features': ['auto', 'sqrt'],
'bootstrap': [True, False]
}

"""##RandomForestRegressor"""

random_search_testing_scores_pca = []
MLSE_scores_pca = []

rfc_pca = RandomForestRegressor()
random_search_pca_rfc = RandomizedSearchCV(rfc_pca, param_distributions = param_dist, n_iter = 10, scoring = 'r2', n_jobs = 1, cv = 5, verbose = 5)
random_search_pca_rfc.fit(x_train, y_train)

random_search_pca_rfc.best_estimator_

random_search_pca_rfc.best_params_

preds = random_search_pca_rfc.predict(x_test)
testing_score = max(0 , 100 * r2_score(y_test , preds))
mlse = max(0,mean_squared_log_error(y_test,preds))
print(f"The testing score by applying tuned RandomForestRegressor is : {testing_score:.4f}")
random_search_testing_scores_pca.append(testing_score)
MLSE_scores_pca.append(mlse)

"""##LGBMRegressor"""

params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ,9],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
}

lgb_pca = lgbm.LGBMRegressor()
random_search_pca_lgb = RandomizedSearchCV(lgb_pca, param_distributions = params, n_iter = 8, scoring = 'r2', n_jobs = 1, cv = 5, verbose = 3)

random_search_pca_lgb.fit(x_train,y_train)

random_search_pca_lgb.best_estimator_

preds_lgbm_pca = random_search_pca_lgb.predict(x_test)
testing_score_lgbm_pca = max(0 , 100 * r2_score(y_test , preds_lgbm_pca))
mlse = max(0,mean_squared_log_error(y_test,preds_lgbm_pca))
print(f"The testing score by applying tuned LgbmRegressor is : {testing_score_lgbm_pca:.4f}")
random_search_testing_scores_pca.append(testing_score_lgbm_pca)
MLSE_scores_pca.append(mlse)

params={
 "learning_rate"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,
 "max_depth"        : [ 3, 4, 5, 6, 8, 10, 12, 15],
 "min_child_weight" : [ 1, 3, 5, 7 ,9],
 "gamma"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],
 "colsample_bytree" : [ 0.3, 0.4, 0.5 , 0.7 ]
}

"""##XGBRegressor"""

xgb_pca = XGBRegressor()
random_search_pca_xgb = RandomizedSearchCV(xgb_pca, param_distributions = params, n_iter = 8, scoring = 'r2', n_jobs = 1, cv = 5, verbose = 3)

random_search_pca_xgb.fit(x_train, y_train)

preds_xgb_pca = random_search_pca_xgb.predict(x_test)
testing_score_xgb_pca = max(0 , 100 * r2_score(y_test , preds_xgb_pca))
mlse = max(0,mean_squared_log_error(y_test,preds_xgb_pca))
print(f"The testing score by applying tuned XgbRegressor is : {testing_score_xgb_pca:.4f}")
random_search_testing_scores_pca.append(testing_score_xgb_pca)
MLSE_scores_pca.append(mlse)

"""##Comapring all three models after applying PCA"""

final_testing_score = pd.DataFrame({'Model': ['RandomForestRegressor','LightGBM','XGBRegressor'], 'R2-Score': random_search_testing_scores_pca, 'MLSE' : MLSE_scores_pca})
final_testing_score.style.background_gradient(high=1,axis=0)

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "R2-Score" , data = final_testing_score)
plt.title("Model Comparision based on R2-Score after applying PCA based on R2- score");

plt.figure(figsize = (10 , 5))
sns.barplot(y = "Model" , x = "MLSE" , data = final_testing_score)
plt.title("Model Comparision based on MLSE after applying PCA based on MLSE");

"""##Final Price Prediction

Since we have seen that without applying pca and after applying pca our XGBregreesor and lgbmregressor are giving us maximum r2-score so let's the final price be predicted by the average of the two price predicted by these models.
"""

final_predicted_price = []
for i in range(len(preds_xgb_pca)):
  final_predicted_price.append((preds_xgb_pca[i] + preds_lgbm_pca[i]) / 2)

final_testing_score = max(0 , 100 * r2_score(y_test , final_predicted_price))
print(f"The testing score by applying tuned combination of XGB Regressor and LGBM Regressor is : {final_testing_score:.4f}")

"""#<h2 align = center><strong> Pipeline </strong></h2>
<img src = "https://github.com/Shrut26/Flight-Ticket-Price-Prediction/blob/main/pipeline.png?raw=true" />

"""

def Pipeline(X,Y):
  #data extraction and preprocessing are exactly the same done their headings
  pca = PCA()
  pca.fit(X) 
  values = pca.transform(X)
  pca_data = pd.DataFrame(columns = ['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6','PCA7','PCA8','PCA9','PCA10'], data = values[:,:10])
  pca_data['Price'] = Y
  final_predicted_price = []
  for i in range(len(preds_xgb_pca)):
    final_predicted_price.append((preds_xgb_pca[i] + preds_lgbm_pca[i]) / 2)
  final_testing_score = max(0 , 100 * r2_score(y_test , final_predicted_price))
  print(f"The testing score by applying tuned combination of XGB Regressor and LGBM Regressor is : {final_testing_score:.4f}")

Pipeline(X,Y)